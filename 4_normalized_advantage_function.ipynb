{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KCZmQKLpBtFQ"
      },
      "source": [
        "## DQN for continuous action spaces: Normalized Advantage Function (NAF)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\pytorch_lightning\\utilities\\imports.py:22: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n",
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\pytorch_lightning\\__init__.py:38: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  __import__(\"pkg_resources\").declare_namespace(__name__)\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import gym\n",
        "import random\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WMKrYHMnFISO"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pGAOZKhbBu_z"
      },
      "outputs": [],
      "source": [
        "class NafDQN(nn.Module):\n",
        "    \n",
        "  def __init__(self, hidden_size, obs_size, action_dims, max_action):\n",
        "    super().__init__()\n",
        "    self.action_dims = action_dims\n",
        "    self.max_action = torch.from_numpy(max_action).to(device)\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(obs_size, hidden_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_size, hidden_size),\n",
        "      nn.ReLU(),   \n",
        "    )\n",
        "    self.linear_mu = nn.Linear(hidden_size, action_dims)\n",
        "    self.linear_value = nn.Linear(hidden_size, 1)\n",
        "    self.linear_matrix = nn.Linear(hidden_size, int(action_dims * (action_dims + 1) / 2))\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def mu(self, x):\n",
        "    x = self.net(x)\n",
        "    x = self.linear_mu(x)\n",
        "    x = torch.tanh(x) * self.max_action\n",
        "    return x\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def value(self, x):\n",
        "    x = self.net(x)\n",
        "    x = self.linear_value(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, a):\n",
        "    x = self.net(x)\n",
        "    mu = torch.tanh(self.linear_mu(x)) * self.max_action\n",
        "    value = self.linear_value(x)\n",
        "    matrix = torch.tanh(self.linear_matrix(x))\n",
        "    \n",
        "    L = torch.zeros((x.shape[0], self.action_dims, self.action_dims)).to(device)\n",
        "    tril_indices = torch.tril_indices(row=self.action_dims, col=self.action_dims, offset=0).to(device)\n",
        "\n",
        "    L[:, tril_indices[0], tril_indices[1]] = matrix\n",
        "    L.diagonal(dim1=1,dim2=2).exp_()\n",
        "    P = L * L.transpose(2, 1)\n",
        "    \n",
        "    u_mu = (a-mu).unsqueeze(dim=1)\n",
        "    u_mu_t = u_mu.transpose(1, 2)\n",
        "    \n",
        "    adv = - 1/2 * u_mu @ P @ u_mu_t\n",
        "    adv = adv.squeeze(dim=-1)\n",
        "    return value + adv\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hziRGjJ9Pkv1"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5B223HLzBvCx"
      },
      "outputs": [],
      "source": [
        "def noisy_policy(state, env, net, epsilon=0.0):\n",
        "  state = torch.tensor([state]).to(device)\n",
        "  amin = torch.from_numpy(env.action_space.low).to(device)\n",
        "  amax = torch.from_numpy(env.action_space.high).to(device)\n",
        "  mu = net.mu(state)\n",
        "  mu = mu + torch.normal(0, epsilon, mu.size(), device=device)\n",
        "  action = mu.clamp(amin, amax)\n",
        "  action = action.squeeze().cpu().numpy()\n",
        "  return action"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yeo3s-QPnZH"
      },
      "source": [
        "#### Create the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fw-77TRyBvHz"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "  \n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "  \n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YtGko6LVQaJz"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ihkyoL5WQgGm"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9XQlZy9J-vjt"
      },
      "outputs": [],
      "source": [
        "class RepeatActionWrapper(gym.Wrapper):\n",
        "  def __init__(self, env, n):\n",
        "    super().__init__(env)\n",
        "    self.env = env\n",
        "    self.n = n\n",
        "      \n",
        "  def step(self, action):\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "    for _ in range(self.n):\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    return next_state, total_reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xZ3h8CCOQjGL"
      },
      "outputs": [],
      "source": [
        "def create_environment(name):\n",
        "  env = gym.make(name)\n",
        "  env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x: x % 1000 == 0)\n",
        "  env = RepeatActionWrapper(env, n=8)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b8fKGgFzQ4EX"
      },
      "source": [
        "#### Update the target network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-q-OJaPnBvKf"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J8de_OtyR1oJ"
      },
      "source": [
        "#### Create the Deep Q-Learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "N-tOW8KgBvNZ"
      },
      "outputs": [],
      "source": [
        "class NAFDeepQLearning(LightningModule):\n",
        "                             \n",
        "  def __init__(self, env_name, policy=noisy_policy, capacity=100_000, \n",
        "               batch_size=256, lr=1e-4, hidden_size=512, gamma=0.99, \n",
        "               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=2.0, eps_end=0.2, \n",
        "               eps_last_episode=1_000, samples_per_epoch=1_000, tau=0.01):\n",
        "\n",
        "    super().__init__()\n",
        "    self.env = create_environment(env_name)\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[0]\n",
        "    action_dims = self.env.action_space.shape[0]\n",
        "    max_action = self.env.action_space.high\n",
        "\n",
        "    self.q_net = NafDQN(hidden_size, obs_size, action_dims, max_action).to(device)\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "    self.policy = policy\n",
        "\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episode(epsilon=self.hparams.eps_start)\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None, epsilon=0.):\n",
        "    obs = self.env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      if policy:\n",
        "        action = policy(obs, self.env, self.q_net, epsilon=epsilon)\n",
        "      else:\n",
        "        action = self.env.action_space.sample()\n",
        "        \n",
        "      next_obs, reward, done, info = self.env.step(action)\n",
        "      exp = (obs, action, reward, done, next_obs)\n",
        "      self.buffer.append(exp)\n",
        "      obs = next_obs\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = self.q_net(x)\n",
        "    return output\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net_optimizer]\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size,\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "\n",
        "    action_values = self.q_net(states, actions)\n",
        "\n",
        "    next_state_values = self.target_q_net.value(next_states)\n",
        "    next_state_values[dones] = 0.0\n",
        "    \n",
        "    target = rewards + self.hparams.gamma * next_state_values\n",
        "\n",
        "    loss = self.hparams.loss_fn(action_values, target)\n",
        "    self.log('episode/MSE Loss', loss)\n",
        "    return loss\n",
        "\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "\n",
        "    epsilon = max(\n",
        "        self.hparams.eps_end,\n",
        "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "    )\n",
        "\n",
        "    self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "    \n",
        "    polyak_average(self.q_net, self.target_q_net, tau=self.hparams.tau)\n",
        "    \n",
        "    self.log(\"episode/Return\", self.env.return_queue[-1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TCusNrF-SPHP"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOhCyJgTBvQR"
      },
      "outputs": [],
      "source": [
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A-fecCQPSVD6"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yY3-mV12BvTK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\gym\\core.py:329: DeprecationWarning: WARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\n",
            "  deprecation(\n",
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: WARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\n",
            "  deprecation(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 samples in experience buffer. Filling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:78: DeprecationWarning: WARN: Recording ability for environment LunarLanderContinuous-v2 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\n",
            "  logger.deprecation(\n",
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\gym\\core.py:51: DeprecationWarning: WARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\n",
            "  deprecation(\n",
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17 samples in experience buffer. Filling...\n",
            "32 samples in experience buffer. Filling...\n",
            "42 samples in experience buffer. Filling...\n",
            "55 samples in experience buffer. Filling...\n",
            "64 samples in experience buffer. Filling...\n",
            "75 samples in experience buffer. Filling...\n",
            "88 samples in experience buffer. Filling...\n",
            "99 samples in experience buffer. Filling...\n",
            "113 samples in experience buffer. Filling...\n",
            "125 samples in experience buffer. Filling...\n",
            "138 samples in experience buffer. Filling...\n",
            "149 samples in experience buffer. Filling...\n",
            "159 samples in experience buffer. Filling...\n",
            "176 samples in experience buffer. Filling...\n",
            "191 samples in experience buffer. Filling...\n",
            "209 samples in experience buffer. Filling...\n",
            "223 samples in experience buffer. Filling...\n",
            "235 samples in experience buffer. Filling...\n",
            "244 samples in experience buffer. Filling...\n",
            "256 samples in experience buffer. Filling...\n",
            "267 samples in experience buffer. Filling...\n",
            "281 samples in experience buffer. Filling...\n",
            "293 samples in experience buffer. Filling...\n",
            "303 samples in experience buffer. Filling...\n",
            "318 samples in experience buffer. Filling...\n",
            "333 samples in experience buffer. Filling...\n",
            "344 samples in experience buffer. Filling...\n",
            "353 samples in experience buffer. Filling...\n",
            "374 samples in experience buffer. Filling...\n",
            "394 samples in experience buffer. Filling...\n",
            "410 samples in experience buffer. Filling...\n",
            "425 samples in experience buffer. Filling...\n",
            "447 samples in experience buffer. Filling...\n",
            "458 samples in experience buffer. Filling...\n",
            "468 samples in experience buffer. Filling...\n",
            "481 samples in experience buffer. Filling...\n",
            "495 samples in experience buffer. Filling...\n",
            "513 samples in experience buffer. Filling...\n",
            "526 samples in experience buffer. Filling...\n",
            "543 samples in experience buffer. Filling...\n",
            "550 samples in experience buffer. Filling...\n",
            "567 samples in experience buffer. Filling...\n",
            "579 samples in experience buffer. Filling...\n",
            "590 samples in experience buffer. Filling...\n",
            "603 samples in experience buffer. Filling...\n",
            "630 samples in experience buffer. Filling...\n",
            "646 samples in experience buffer. Filling...\n",
            "660 samples in experience buffer. Filling...\n",
            "672 samples in experience buffer. Filling...\n",
            "685 samples in experience buffer. Filling...\n",
            "700 samples in experience buffer. Filling...\n",
            "712 samples in experience buffer. Filling...\n",
            "723 samples in experience buffer. Filling...\n",
            "746 samples in experience buffer. Filling...\n",
            "766 samples in experience buffer. Filling...\n",
            "782 samples in experience buffer. Filling...\n",
            "794 samples in experience buffer. Filling...\n",
            "806 samples in experience buffer. Filling...\n",
            "827 samples in experience buffer. Filling...\n",
            "844 samples in experience buffer. Filling...\n",
            "854 samples in experience buffer. Filling...\n",
            "867 samples in experience buffer. Filling...\n",
            "879 samples in experience buffer. Filling...\n",
            "888 samples in experience buffer. Filling...\n",
            "901 samples in experience buffer. Filling...\n",
            "917 samples in experience buffer. Filling...\n",
            "928 samples in experience buffer. Filling...\n",
            "938 samples in experience buffer. Filling...\n",
            "949 samples in experience buffer. Filling...\n",
            "957 samples in experience buffer. Filling...\n",
            "971 samples in experience buffer. Filling...\n",
            "981 samples in experience buffer. Filling...\n",
            "996 samples in experience buffer. Filling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: c:\\Users\\aksha\\OneDrive\\Desktop\\rl adv\\proj 2\\lightning_logs\n",
            "\n",
            "  | Name         | Type   | Params\n",
            "----------------------------------------\n",
            "0 | q_net        | NafDQN | 270 K \n",
            "1 | target_q_net | NafDQN | 270 K \n",
            "----------------------------------------\n",
            "540 K     Trainable params\n",
            "0         Non-trainable params\n",
            "540 K     Total params\n",
            "2.163     Total estimated model params size (MB)\n",
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: : 4it [00:00, 43.26it/s, loss=29.8, v_num=0]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_29700\\2395586982.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
            "  state = torch.tensor([state]).to(device)\n",
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:32: DeprecationWarning: This property will be removed in 2.0.0. Use `Metric.updated_called` instead.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: : 4it [00:03,  1.14it/s, loss=24.6, v_num=0]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:78: DeprecationWarning: WARN: Recording ability for environment LunarLanderContinuous-v2 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\n",
            "  logger.deprecation(\n",
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\gym\\core.py:51: DeprecationWarning: WARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\n",
            "  deprecation(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 573: : 4it [02:03, 30.78s/it, loss=22.5, v_num=0] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aksha\\anaconda2\\envs\\envgym\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:727: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "algo = NAFDeepQLearning('LunarLanderContinuous-v2')\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=num_gpus, \n",
        "    max_epochs=np.inf\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
